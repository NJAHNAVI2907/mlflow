# MLflow Tracing V2 vs V3 Architecture

## Overview

MLflow has two trace formats that are automatically selected based on the tracking URI:
- **V2 traces**: Used for local and non-Databricks tracking servers
- **V3 traces**: Used for Databricks environments

## Key Architectural Difference: Trace ID Generation

The fundamental difference between V2 and V3 is **when and how trace IDs are generated**:

### V2 Trace ID Generation (Server-Side)
- Uses `generate_request_id_v2()` which creates a random UUID hex string
- Generated **server-side** during backend API call
- **Requires early backend interaction** to obtain the trace ID
- Forces a two-phase approach: start trace → collect spans → end trace

```python
# V2: Must call backend first to get trace ID
trace_info = self._client.start_trace(
    experiment_id=experiment_id,
    timestamp_ms=start_time,
    # ... other metadata
)
# trace_info.request_id is generated by backend
```

### V3 Trace ID Generation (Client-Side)
- Uses `generate_trace_id_v3(span)` which prefixes OpenTelemetry's trace ID with "tr-"
- Generated **client-side** using existing OTel trace context
- **No backend interaction needed** for ID generation
- Enables atomic trace creation: collect complete trace → send to backend

```python
# V3: Generate trace ID locally using OTel trace ID
trace_id = generate_trace_id_v3(root_span)  # "tr-" + otel_trace_id
trace_info = TraceInfo(trace_id=trace_id, ...)
# No backend call needed until export
```

## Why V2 Needed Two Phases

1. **Backend Dependency**: V2 trace IDs require a backend API call to generate
2. **Timing Constraint**: When a root span starts, we need a trace ID but the backend must provide it
3. **Incomplete Information**: At span start, we don't have child spans, final duration, or complete metadata
4. **Performance Impact**: Backend API call adds ~1 second latency (noted in code comments)

## Why V3 Enables Atomic Creation

1. **Client-Side Generation**: Trace ID available immediately using OTel trace context
2. **Deferred Backend Interaction**: No backend calls until trace is complete
3. **Export-Time Creation**: `start_trace_v3()` called when all spans are collected
4. **Complete Data Model**: Full `Trace` object (info + data) created atomically

## Implementation Details

### V2 Flow
```
Root Span Starts → Backend API Call (get trace ID) → Create TraceInfoV2 → Collect Spans → Root Span Ends → Backend API Call (update trace) → Export Complete
```

### V3 Flow
```
Root Span Starts → Generate Trace ID Locally → Collect Spans → Root Span Ends → Create Complete Trace → Backend API Call (atomic) → Export Complete
```

## Performance Implications

- **V2**: Adds ~1 second latency at trace start due to backend API call
- **V3**: No additional latency - trace ID generation is instantaneous

## Span Format Differences

Beyond trace ID generation, **span formats themselves differ significantly**:

### V2 Span Schema
```json
{
  "name": "test_span",
  "context": {
    "span_id": "8a90fc46e65ea5a4",          // Hex string
    "trace_id": "0125978dc5c5a..."
  },
  "parent_id": "parent_span_id_or_null",
  "start_time": 1738662897576578992,
  "end_time": 1738662899068969049,
  "status_code": "OK",                      // Simple string
  "status_message": "",
  "attributes": {...}
}
```

### V3 Span Schema  
```json
{
  "name": "test_span",
  "trace_id": "base64_encoded_bytes",       // Base64 bytes (OTLP)
  "span_id": "base64_encoded_bytes",
  "parent_span_id": "base64_encoded_bytes_or_empty",
  "start_time_unix_nano": 1738662897576578992,
  "end_time_unix_nano": 1738662899068969049,
  "status": {                               // OTLP status object
    "code": "STATUS_CODE_OK",
    "message": ""
  },
  "trace_state": "",                        // W3C compliance
  "attributes": {...}
}
```

### Key Differences
- **ID Encoding**: V2 uses hex strings, V3 uses base64-encoded bytes
- **Schema Structure**: V2 uses nested `context`, V3 uses flat structure  
- **Field Names**: Different naming conventions (`start_time` vs `start_time_unix_nano`)
- **Status Format**: V2 simple strings, V3 OTLP enum values
- **OTLP Compliance**: V3 is OpenTelemetry Protocol compatible, V2 is custom format

### Backward Compatibility
V3 automatically detects and converts V2 spans:
```python
@staticmethod
def _is_span_v2_schema(data: dict[str, Any]) -> bool:
    return "context" in data
```

## Current Backend Support Status

**The key limitation**: Only Databricks backend currently supports V3 traces!

### Backend Selection Logic
```python
# In mlflow/tracing/provider.py
if is_databricks_uri(tracking_uri):
    # Uses V3: MlflowV3SpanExporter + MlflowV3SpanProcessor
else:
    # Uses V2: MlflowV2SpanExporter + MlflowV2SpanProcessor
```

### V3 Support by Backend

| Backend | V3 Support | Status |
|---------|------------|---------|
| **RestStore (Databricks)** | ✅ Full | `start_trace_v3()`, V3 search, blob storage |
| **FileStore (OSS)** | ❌ None | Raises "not supported" exception |
| **SQLAlchemyStore (OSS)** | ❌ None | Raises "not supported" exception |
| **RestStore (Non-Databricks)** | ❌ None | Uses V2 processor |

### What's Missing for OSS V3 Support

**Required Implementations:**
1. `start_trace_v3(trace: Trace) -> TraceInfo` in FileStore/SQLAlchemyStore
2. V3 search support (`search_traces()` with V3 schema)
3. V3 trace retrieval (`get_trace_info()` with V3 support)
4. Blob storage integration for trace data
5. Database schema updates for V3 metadata

**Key Blocker**: OSS backends lack fundamental V3 API methods and blob storage integration that V3 traces require.

## RestStore V3 Migration Plan

**Key Insight**: RestStore is just the HTTP client - the real work is in the storage backends!

**Migration Flow**: 
```
RestStore (client) → HTTP → Tracking Server → SQLAlchemyStore/FileStore (actual storage)
```

### Detailed Migration Plan

## Migration Approach Analysis

### **Approach 1: New Table (Original Plan)**

**Pros:**
- ✅ **Zero migration cost** - existing traces untouched
- ✅ **No downtime** during migration  
- ✅ **Risk-free rollback** - can drop new table without affecting V2
- ✅ **Clear separation** - V2 and V3 coexist independently

**Cons:**
- ❌ **Dual maintenance** - need to maintain both table structures
- ❌ **Code complexity** - routing logic between V2/V3 tables
- ❌ **Storage overhead** - two tables for similar data

### **Approach 2: Schema Migration (Recommended)**

**Pros:**
- ✅ **Single table** - unified trace storage
- ✅ **Simpler codebase** - one table structure to maintain  
- ✅ **Unified queries** - can search all traces together
- ✅ **Cleaner long-term** - eventual migration to V3 only

**Cons:**
- ❌ **Migration cost** - potentially expensive for large datasets
- ❌ **Downtime risk** - schema migration might lock table
- ❌ **Rollback complexity** - harder to revert if issues

### **Migration Cost Analysis**

**Current V2 Schema:**
```sql
trace_info: request_id, experiment_id, timestamp_ms, execution_time_ms, status
trace_tags: request_id, key, value
trace_request_metadata: request_id, key, value
```

**Required V3 Additions:**
```sql
-- Add columns to existing trace_info table
ALTER TABLE trace_info ADD COLUMN trace_id VARCHAR(255);
ALTER TABLE trace_info ADD COLUMN request_time BIGINT;
ALTER TABLE trace_info ADD COLUMN execution_duration BIGINT; 
ALTER TABLE trace_info ADD COLUMN state VARCHAR(50);
ALTER TABLE trace_info ADD COLUMN trace_location_type VARCHAR(50);
ALTER TABLE trace_info ADD COLUMN trace_location_id VARCHAR(255);
ALTER TABLE trace_info ADD COLUMN request_preview TEXT;
ALTER TABLE trace_info ADD COLUMN response_preview TEXT;
ALTER TABLE trace_info ADD COLUMN client_request_id VARCHAR(255);

-- Data migration for existing records
UPDATE trace_info SET 
    trace_id = CONCAT('tr-', request_id),
    request_time = timestamp_ms,
    execution_duration = execution_time_ms,
    state = status,
    trace_location_type = 'experiment',
    trace_location_id = CAST(experiment_id AS VARCHAR);
```

**Migration Performance:**
- **Low Risk**: < 10K traces (~seconds)
- **Medium Risk**: 10K-100K traces (~minutes, may lock table)
- **High Risk**: > 100K traces (~hours, significant downtime)

#### **Recommended: Approach 2 with Optimizations**

**1.1 Database Schema Migration (Optimized)**

```python
# mlflow/store/db_migrations/versions/xxx_migrate_traces_to_v3_schema.py

def upgrade():
    # Step 1: Add new columns (fast, no table lock)
    op.add_column("trace_info", sa.Column("trace_id", sa.String(255), nullable=True))
    op.add_column("trace_info", sa.Column("request_time", sa.BigInteger, nullable=True))
    op.add_column("trace_info", sa.Column("execution_duration", sa.BigInteger, nullable=True))
    op.add_column("trace_info", sa.Column("state", sa.String(50), nullable=True))
    op.add_column("trace_info", sa.Column("trace_location_type", sa.String(50), nullable=True))
    op.add_column("trace_info", sa.Column("trace_location_id", sa.String(255), nullable=True))
    op.add_column("trace_info", sa.Column("request_preview", sa.Text, nullable=True))
    op.add_column("trace_info", sa.Column("response_preview", sa.Text, nullable=True))
    op.add_column("trace_info", sa.Column("client_request_id", sa.String(255), nullable=True))
    
    # Step 2: Populate V3 data for existing records (batched to avoid locks)
    connection = op.get_bind()
    
    # Process in batches of 1000 to avoid long-running transactions
    batch_size = 1000
    offset = 0
    
    while True:
        result = connection.execute(
            text(f"""
            UPDATE trace_info SET 
                trace_id = CONCAT('tr-', request_id),
                request_time = timestamp_ms,
                execution_duration = execution_time_ms,
                state = status,
                trace_location_type = 'experiment',
                trace_location_id = CAST(experiment_id AS VARCHAR(255)),
                client_request_id = request_id
            WHERE trace_id IS NULL 
            LIMIT {batch_size}
            """)
        )
        
        if result.rowcount == 0:
            break
        offset += batch_size
    
    # Step 3: Add constraints after data migration
    op.create_index("index_trace_info_trace_id", "trace_info", ["trace_id"], unique=True)
```

**1.2 Backward Compatibility Design**
```python
# Keep both V2 and V3 columns during transition period
# V2 fields: request_id, timestamp_ms, execution_time_ms, status
# V3 fields: trace_id, request_time, execution_duration, state

# Auto-populate both formats for new traces
def create_trace_record(self, trace_info):
    if hasattr(trace_info, 'trace_id'):  # V3 trace
        return {
            # V3 fields
            'trace_id': trace_info.trace_id,
            'request_time': trace_info.request_time,
            'execution_duration': trace_info.execution_duration,
            'state': trace_info.state.value,
            # V2 compatibility fields
            'request_id': trace_info.trace_id.replace('tr-', ''),
            'timestamp_ms': trace_info.request_time,
            'execution_time_ms': trace_info.execution_duration,
            'status': trace_info.state.value,
        }
```

**1.3 SQLAlchemyStore Method Implementation**
```python
def start_trace_v3(self, trace: Trace) -> TraceInfo:
    # 1. Store trace metadata in traces_v3 table
    # 2. Upload trace data to artifact storage
    # 3. Return TraceInfo with artifact path
```

**1.3 Search and Retrieval Support**
```python
def search_traces(self, experiment_ids, filter_string, max_results, order_by, page_token, should_query_v3=False):
    # Add V3 query path when should_query_v3=True
    
def get_trace_info(self, trace_id, should_query_v3=False):
    # Add V3 retrieval path
```

#### Step 2: FileStore V3 Implementation

**2.1 File Storage Format Design**
```
mlruns/
├── 0/                          # experiment_id
│   ├── traces_v3/              # V3 traces directory
│   │   ├── tr-abc123.../       # trace_id directory
│   │   │   ├── meta.json       # TraceInfo metadata
│   │   │   └── data.json       # TraceData (spans)
```

**2.2 FileStore Method Implementation**
```python
def start_trace_v3(self, trace: Trace) -> TraceInfo:
    # 1. Create trace directory
    # 2. Write meta.json with TraceInfo
    # 3. Write data.json with TraceData
    # 4. Return TraceInfo
```

#### Step 3: Tracking Server V3 Endpoints

**3.1 Add V3 Handlers**
```python
# In mlflow/server/handlers.py
@app.route("/api/3.0/mlflow/traces", methods=["POST"])
def start_trace_v3():
    request_message = _get_request_message(StartTraceV3Request())
    trace = _get_trace_from_request(request_message)
    response = _get_tracking_store().start_trace_v3(trace)
    return _get_response_message(response)
```

**3.2 Route Registration**
```python
# Add V3 routes to routing table
TRACE_V3_ROUTES = [
    ("POST", "/api/3.0/mlflow/traces", start_trace_v3),
    ("GET", "/api/3.0/mlflow/traces", search_traces_v3),
    ("GET", "/api/3.0/mlflow/traces/<trace_id>", get_trace_v3),
]
```

#### Step 4: Artifact Storage Integration

**4.1 Trace Data Artifact Management**
```python
class TraceArtifactRepository:
    def upload_trace_data(self, trace_id: str, trace_data: TraceData) -> str:
        # Upload to artifact storage, return artifact path
        
    def download_trace_data(self, artifact_path: str) -> TraceData:
        # Download from artifact storage, return TraceData
```

**4.2 Store Integration**
```python
# In both SQLAlchemyStore and FileStore
def start_trace_v3(self, trace: Trace) -> TraceInfo:
    # Upload trace data to artifacts
    artifact_path = self.artifact_repo.upload_trace_data(trace.info.trace_id, trace.data)
    
    # Store metadata with artifact path reference
    trace_info = TraceInfo(
        trace_id=trace.info.trace_id,
        # ... other metadata
        artifact_path=artifact_path
    )
```

#### Step 5: Provider Logic Update

**5.1 Enable V3 for OSS**
```python
# In mlflow/tracing/provider.py
def _get_mlflow_span_processor():
    # Remove Databricks-only restriction
    # if is_databricks_uri(tracking_uri):  # Remove this condition
    
    # Always prefer V3 when available
    if _tracking_store_supports_v3():
        return MlflowV3SpanProcessor()
    else:
        return MlflowV2SpanProcessor()
```

#### Step 6: Backward Compatibility

**6.1 V2 to V3 Migration**
```python
def migrate_v2_to_v3(store):
    # Optional: migrate existing V2 traces to V3 format
    # Convert TraceInfoV2 → TraceInfo
    # Convert request_id → trace_id with "tr-" prefix
```

**6.2 Dual Support**
```python
# Support both V2 and V3 simultaneously
def get_trace_info(self, trace_id, should_query_v3=None):
    if should_query_v3 is None:
        # Auto-detect based on trace_id format
        should_query_v3 = trace_id.startswith("tr-")
    
    if should_query_v3:
        return self._get_trace_info_v3(trace_id)
    else:
        return self._get_trace_info_v2(trace_id)
```

### Migration Benefits
- **Atomic trace creation** (no more start/end phases)
- **OTLP compliance** (industry standard)  
- **Better performance** (single API call)
- **Richer metadata** (trace locations, previews)
- **Better storage efficiency** (separate metadata/data)

## Files to Reference

- Trace ID generation: `mlflow/tracing/utils/__init__.py`
- V2 processor: `mlflow/tracing/processor/mlflow_v2.py`
- V3 processor: `mlflow/tracing/processor/mlflow_v3.py`
- Provider logic: `mlflow/tracing/provider.py`
- Span conversion: `mlflow/entities/span.py`